{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EthioMart/notebooks/ner_model_training.ipynb\n",
    "\n",
    "# --- Section 1: Setup and Configuration ---\n",
    "\n",
    "# 1.1 Import necessary libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project root to the system path to allow importing from src and config\n",
    "project_root = Path.cwd().parent # This assumes you run the notebook from EthioMart/notebooks/\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import the custom labeler and configuration\n",
    "try:\n",
    "    from src.data_labeler import AmharicRuleBasedLabeler\n",
    "    from config.config import DATA_DIR\n",
    "    print(\"✅ Successfully imported AmharicRuleBasedLabeler and DATA_DIR.\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing modules: {e}\")\n",
    "    print(\"Please ensure your `src` directory and `config.config` are set up correctly.\")\n",
    "    # Fallback paths for local testing if config import fails\n",
    "    DATA_DIR = Path(\"../data\") # Fallback for data directory\n",
    "\n",
    "# Define paths for input and output\n",
    "# Input: The CoNLL file generated by data_labeler.py\n",
    "CONLL_INPUT_PATH = DATA_DIR / \"annotated\" / \"telegram_ner_data_rule_based.conll\"\n",
    "\n",
    "# Output: Binary SpaCy format (.spacy) for training\n",
    "SPACY_TRAIN_DATA_PATH = DATA_DIR / \"processed\" / \"train_ner_data.spacy\"\n",
    "SPACY_DEV_DATA_PATH = DATA_DIR / \"processed\" / \"dev_ner_data.spacy\"\n",
    "SPACY_TEST_DATA_PATH = DATA_DIR / \"processed\" / \"test_ner_data.spacy\"\n",
    "\n",
    "\n",
    "# --- Section 2: Generate CoNLL Data (if not already done) ---\n",
    "\n",
    "print(\"\\n--- Section 2: Generate CoNLL Data ---\")\n",
    "if not CONLL_INPUT_PATH.exists() or os.path.getsize(CONLL_INPUT_PATH) == 0:\n",
    "    print(f\"❗ {CONLL_INPUT_PATH} not found or is empty. Running rule-based labeler...\")\n",
    "    \n",
    "    # Ensure the input CSV for the labeler exists\n",
    "    cleaned_csv_path = DATA_DIR / \"processed\" / \"clean_telegram_data.csv\"\n",
    "    if not cleaned_csv_path.exists():\n",
    "        print(f\"❌ Error: Cleaned data CSV not found at {cleaned_csv_path}. Please run preprocessor.py first.\")\n",
    "    else:\n",
    "        labeler = AmharicRuleBasedLabeler()\n",
    "        success = labeler.process_to_conll(\n",
    "            input_csv_path=str(cleaned_csv_path),\n",
    "            output_conll_path=str(CONLL_INPUT_PATH),\n",
    "            sample_size=None # Use None for full dataset, or a number for testing\n",
    "        )\n",
    "        if success:\n",
    "            print(\"✅ CoNLL data generation complete.\")\n",
    "        else:\n",
    "            print(\"❌ CoNLL data generation failed. Check logs above.\")\n",
    "else:\n",
    "    print(f\"✅ CoNLL data found at {CONLL_INPUT_PATH}. Skipping regeneration.\")\n",
    "\n",
    "# --- Section 3: Load and Inspect CoNLL Data ---\n",
    "\n",
    "print(\"\\n--- Section 3: Load and Inspect CoNLL Data ---\")\n",
    "\n",
    "# A simple function to read CoNLL-like data\n",
    "def read_conll(file_path):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line: # If line is not empty\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) == 2:\n",
    "                        current_sentence.append((parts[0], parts[1]))\n",
    "                    else:\n",
    "                        print(f\"Warning: Skipping malformed line in CoNLL file: '{line}'\")\n",
    "                else: # Empty line signifies end of a sentence\n",
    "                    if current_sentence:\n",
    "                        sentences.append(current_sentence)\n",
    "                        current_sentence = []\n",
    "            if current_sentence: # Add the last sentence if file doesn't end with a newline\n",
    "                sentences.append(current_sentence)\n",
    "        print(f\"Loaded {len(sentences)} sentences from {file_path}\")\n",
    "        return sentences\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CoNLL file not found at {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CoNLL file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load the data\n",
    "all_sentences = read_conll(CONLL_INPUT_PATH)\n",
    "\n",
    "if not all_sentences:\n",
    "    print(\"No sentences loaded. Cannot proceed with data splitting and training setup.\")\n",
    "else:\n",
    "    # Display a sample sentence with its tags\n",
    "    print(\"\\n--- Sample Annotated Sentence ---\")\n",
    "    for i, (token, tag) in enumerate(all_sentences[0]):\n",
    "        if i < 10: # Display first 10 tokens for brevity\n",
    "            print(f\"{token}\\t{tag}\")\n",
    "        elif i == 10:\n",
    "            print(\"...\")\n",
    "\n",
    "\n",
    "# --- Section 4: Convert to SpaCy DocBin and Split Data ---\n",
    "\n",
    "print(\"\\n--- Section 4: Convert to SpaCy DocBin and Split Data ---\")\n",
    "\n",
    "# Initialize a blank SpaCy model for tokenization\n",
    "nlp_blank = spacy.blank(\"xx\")\n",
    "\n",
    "# Convert sentences to SpaCy Doc objects\n",
    "docs = []\n",
    "for sent_tokens_tags in tqdm(all_sentences, desc=\"Converting to SpaCy Docs\"):\n",
    "    words = [item[0] for item in sent_tokens_tags]\n",
    "    tags = [item[1] for item in sent_tokens_tags]\n",
    "    \n",
    "    doc = nlp_blank(u\" \".join(words)) # Create a doc from joined words, then re-align tokens\n",
    "    \n",
    "    # Re-align tokens with original words and assign entity tags\n",
    "    # This is a bit tricky with spacy.blank. It's often better to create Doc from `words` and `spaces` directly.\n",
    "    # For now, we'll iterate and set entities if they align perfectly to avoid complexity.\n",
    "    # For robust NER training, it's critical that spans are correctly identified.\n",
    "    \n",
    "    entities = []\n",
    "    # Simplified approach: for each token in the doc, if its text matches the original word\n",
    "    # and has a tag, try to create an entity. This is not how `offsets_to_biluo_tags`\n",
    "    # normally works; it's inverse. We already have BILUO tags.\n",
    "    # What we need to do is apply the BILUO tags to a new Doc.\n",
    "\n",
    "    # Rebuilding Doc with entities from BILUO tags for proper SpaCy training format\n",
    "    # This requires `doc.from_json` or manually adding spans.\n",
    "    # A cleaner way using DocBin:\n",
    "    \n",
    "    # Create an empty Doc and manually add tokens and NER spans\n",
    "    doc = nlp_blank.make_doc(\" \".join(words)) # This creates the tokens based on the joined words\n",
    "    \n",
    "    # We now need to re-align the original tokens from the CoNLL data with the new SpaCy doc's tokens\n",
    "    # and then set the entities based on the BILUO tags.\n",
    "    # This is non-trivial if the tokenizer introduces new splits.\n",
    "    \n",
    "    # A robust way is to use `spacy.tokens.Span` and `doc.set_ents`\n",
    "    # However, for converting CoNLL to DocBin, spacy's `conllu` format is easier,\n",
    "    # or if we have (text, ents) tuples:\n",
    "    \n",
    "    # Let's re-think: The `offsets_to_biluo_tags` approach is typically used to get tags from (text, ents)\n",
    "    # Here, we *have* tokens and BILUO tags. We need to convert this *back* into Spacy's (text, entities) format\n",
    "    # for the DocBin.\n",
    "    \n",
    "    # The `data_labeler.py` already gave us `(start_offset, end_offset, label)`\n",
    "    # We need to create a list of (text, {\"entities\": [(start, end, label)]}) for DocBin.\n",
    "    \n",
    "    # We should re-parse the CoNLL file to get (original_text, entities_list) for `DocBin.add(nlp(text), entities=entities_list)`\n",
    "    # Or, the `read_conll` function should return something more useful.\n",
    "    \n",
    "    # Let's adjust `read_conll` to produce a list of (text, entities)\n",
    "    # The current `read_conll` produces list of [(\"token\", \"TAG\")]\n",
    "    \n",
    "    # Instead of re-reading, let's use the original logic from `data_labeler.py`\n",
    "    # and transform it into a format suitable for SpaCy's DocBin.\n",
    "\n",
    "    # We need a list of (text, annotations) where annotations is a dict like {\"entities\": [(start, end, label)]}\n",
    "    # This conversion logic should ideally be *part* of the labeler script,\n",
    "    # or a separate helper function.\n",
    "\n",
    "    # For this notebook, let's assume we can regenerate (text, ents) from the CoNLL,\n",
    "    # which is less direct. The most robust way is to re-run the `extract_entities`\n",
    "    # from the original preprocessed texts to get correct offsets.\n",
    "    \n",
    "    # Re-loading preprocessed text and regenerating entities for SpaCy DocBin\n",
    "    # This is safer than trying to reverse-engineer offsets from CoNLL.\n",
    "    df_clean = pd.read_csv(DATA_DIR / \"processed\" / \"clean_telegram_data.csv\", encoding='utf-8')\n",
    "    df_clean_filtered = df_clean[df_clean['preprocessed_text'].notna() & (df_clean['preprocessed_text'] != '')]\n",
    "    \n",
    "    labeler_instance = AmharicRuleBasedLabeler() # Re-initialize the labeler for its patterns\n",
    "    \n",
    "    training_data = []\n",
    "    for _, row in tqdm(df_clean_filtered.iterrows(), total=len(df_clean_filtered), desc=\"Preparing SpaCy training data\"):\n",
    "        text = str(row['preprocessed_text'])\n",
    "        entities = labeler_instance._extract_entities_from_text(text)\n",
    "        \n",
    "        # SpaCy DocBin expects entities in (start, end, label) format\n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "    print(f\"Prepared {len(training_data)} samples for SpaCy training.\")\n",
    "\n",
    "    # Split the data into training, development (validation), and test sets\n",
    "    random.seed(42) # For reproducibility\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    train_ratio = 0.8\n",
    "    dev_ratio = 0.1\n",
    "    \n",
    "    train_size = int(len(training_data) * train_ratio)\n",
    "    dev_size = int(len(training_data) * dev_ratio)\n",
    "\n",
    "    train_data = training_data[:train_size]\n",
    "    dev_data = training_data[train_size : train_size + dev_size]\n",
    "    test_data = training_data[train_size + dev_size :]\n",
    "\n",
    "    print(f\"Train samples: {len(train_data)}\")\n",
    "    print(f\"Dev samples: {len(dev_data)}\")\n",
    "    print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "    # Convert to SpaCy's binary DocBin format\n",
    "    def create_docbin(data, nlp_model):\n",
    "        db = DocBin()\n",
    "        for text, annot in tqdm(data, desc=\"Creating DocBin\"):\n",
    "            doc = nlp_model(text)\n",
    "            ents = []\n",
    "            for start, end, label in annot[\"entities\"]:\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "                if span is None:\n",
    "                    # This happens if character offsets don't perfectly align to tokens\n",
    "                    # This is why rule-based regex and SpaCy tokenization need careful alignment\n",
    "                    # For simple cases, 'contract' mode might help. More complex solutions might involve\n",
    "                    # custom tokenization or manual span correction.\n",
    "                    logging.warning(f\"Skipping entity due to non-alignment: '{text[start:end]}' at {start}-{end} with label '{label}' in text '{text}'\")\n",
    "                else:\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "        return db\n",
    "\n",
    "    # Create DocBins\n",
    "    # Use spacy.blank(\"xx\") for creating docs, as we are only using it for tokenization\n",
    "    # and creating annotated Doc objects, not for pre-trained entity recognition.\n",
    "    nlp_for_docbin = spacy.blank(\"xx\") \n",
    "    \n",
    "    train_db = create_docbin(train_data, nlp_for_docbin)\n",
    "    dev_db = create_docbin(dev_data, nlp_for_docbin)\n",
    "    test_db = create_docbin(test_data, nlp_for_docbin)\n",
    "\n",
    "    # Save DocBin files\n",
    "    SPACY_TRAIN_DATA_PATH.parent.mkdir(parents=True, exist_ok=True) # Ensure dir exists\n",
    "    train_db.to_disk(SPACY_TRAIN_DATA_PATH)\n",
    "    dev_db.to_disk(SPACY_DEV_DATA_PATH)\n",
    "    test_db.to_disk(SPACY_TEST_DATA_PATH)\n",
    "\n",
    "    print(f\"\\n✅ SpaCy training data saved to:\")\n",
    "    print(f\"- Train: {SPACY_TRAIN_DATA_PATH}\")\n",
    "    print(f\"- Dev: {SPACY_DEV_DATA_PATH}\")\n",
    "    print(f\"- Test: {SPACY_TEST_DATA_PATH}\")\n",
    "\n",
    "    # --- Section 5: Next Steps: SpaCy Training Command ---\n",
    "\n",
    "    print(\"\\n--- Section 5: Next Steps: SpaCy Training Command ---\")\n",
    "    print(\"You have now prepared your data for SpaCy NER model training.\")\n",
    "    print(\"To train a SpaCy NER model, you typically need a `config.cfg` file.\")\n",
    "    print(\"You can generate a base config file using `spacy init config`:\")\n",
    "    print(\"\\nExample `spacy init config` command:\")\n",
    "    print(f\"cd {project_root}\")\n",
    "    print(f\"spacy init config ./config/config.cfg --lang xx --pipeline ner --optimize efficiency --force\")\n",
    "    print(\"\\nAfter generating and potentially customizing `config.cfg`, you can train your model:\")\n",
    "    print(\"Example `spacy train` command:\")\n",
    "    print(f\"spacy train ./config/config.cfg --output ./models/ner_model --paths.train {SPACY_TRAIN_DATA_PATH} --paths.dev {SPACY_DEV_DATA_PATH}\")\n",
    "    print(\"\\nRemember to install all required dependencies for SpaCy training:\")\n",
    "    print(\"pip install spacy[transformers] # if using transformer-based models\")\n",
    "    print(\"pip install cupy-cuda11x # if you have a CUDA GPU and want GPU acceleration\")\n",
    "    print(\"\\nOnce trained, your model will be saved in `./models/ner_model`.\")\n",
    "    print(\"You can then load it using `nlp = spacy.load('./models/ner_model/model-best')` for inference.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
